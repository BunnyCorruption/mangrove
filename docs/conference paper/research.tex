\begin{center}
V. Research
\end{center}
\begin{flushleft}
\setlength{\parindent}{0.125in}
This project presented a large number of opportunities for research into multiple facets of soundscape ecology, on both the technical and philosophical side. Technologically, we needed to solve a big data issue, where terabytes of information needed to be processed and organized efficiently, in a manner where multiple researchers may be using the same data center. Additionally, processing speeds for sound files using the R language were notoriously slow, so speeding up processing in any way possible was paramount. On the philosophical side, the study of soundscape ecology is relatively new, so the understanding of how the mathematical indices (explained above) correlate to actual meaning is still limited. Further, the validity of these indices when trying to make conclusions as to the ecological health is also up for debate.\\[4pt]

\noindent\textit{A. File Splitting}\par
As the files being used are commonly around an hour long, 1.2 Gb WAV files, the processing time and power needed can be drastic, especially when hundreds of files need to be analyzed concurrently. Thus the idea to split files was considered, for both time and memory efficiency. It was concluded that file splitting did indeed improve processing times, by about 70 seconds between 30 minute and 1 minute splits. However upon further research, it was found that splitting files changes the output values of the indices. This was due to the way the spectrograph analysis is constructed within the algorithm. Inputting different splits of the sound file lead to different sections of the file being sampled. Thus, a completely different output was given and the validity of those results became questionable. It was decided that file splitting functionality would not be included in the job processor in order to preserve data validity.\\[4pt]

\noindent\textit{B. Performance Enhancements}\par
Substantial effort was made to improve the \codesnip{soundecology} package. This effort was directed primarily towards the ACI index. First, the algorithm was analyzed using an R package called \codesnip{benchmarkme}. Along with code profiling, this allowed us to see that most of the processing power was being diverted to \codesnip{for} loops. In order to remedy this situation we took the contents of the \codesnip{for} loop and made functions out of them. With this done, we were able to vectorize those functions. This lead to a 60\% decrease in runtime. With such promising results, future groups may be able to apply this technique to any other indices they see fit.\\[4pt]

\noindent\textit{C. Visualization Research}\par
As each index represents different data in a sound file, it is important to represent that data in a way that makes sense to researchers looking to publish their findings. In general, bar charts and line charts are used in the service to represent the data, but each index has its own meaning in its charts. Regardless of index, a line graph representing the index value over time is available. Being able to show index value over time is of great importance to researchers as change over time is great for drawing conclusions when correlating with other forms of research. A great example of this would be our own sponsor Dr. Beever\textquotesingle s research at the Sanford Zoo. There, they are researching whether the man-made noise like passing trains, has any effect on animal well-being.\\[4pt]

\noindent\textit{D. Data Collection}\par
In order to make headway for future groups to be able to perform machine learning, we needed a great deal of raw data. We had three major sources from which we gathered our data, the primary source being the Cornell Avian laboratory\textquotesingle s database of bird sounds. Along with this package of sound, we used the EC-50 sound data set to supplement our data with anthrophonic sounds. We combined these two data sources with a labeled excel spreadsheet. With this labeled data set we created spectrographs for all the sounds in order to facilitate the use of convolutional networks. Additionally, we web scraped some insect sounds and included code in order to display how to scrape websites. This should allow future teams to collect data from almost any online source.\\[4pt]

\noindent\textit{E. Machine Learning}\par
Many architectures were studied in our journey to find the optimal solution for machine listening. What we found is that convolutional networks out-perform vanilla networks in accuracy. While vanilla networks trained on MFCC log scale sounds only performed with a 60\% accuracy, convolutional networks trained on spectrographs can achieve up to 75\% accuracy. We found that in order to achieve maximum performance you can run the spectrographs through ConvRBMs. These transform regular spectrographs that use simple Fourier transforms to display sounds into representations that have highlight prominent features. This allows convolutional networks to more easily detect these features and correctly label sounds.

\end{flushleft}
